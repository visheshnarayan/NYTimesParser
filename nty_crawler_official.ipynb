{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYTimesParser():\n",
    "    \"\"\"\n",
    "    Class for parsing NYTimes articles using NYTimes sitemaps\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes NYTimesParser obj\n",
    "        \"\"\"\n",
    "\n",
    "    def parse_articles(self, start_year, end_year):\n",
    "        \"\"\"\n",
    "        Parses articles from start_year to end_year; returns pandas.Dataframe with extracted data\n",
    "        \"\"\"\n",
    "\n",
    "        # --------- pandas.Dataframe() where all data is saved --------- #\n",
    "        df = pd.DataFrame(columns= ['date', 'headline', 'author', 'topic', 'text', 'link'])\n",
    "\n",
    "        # -------------- creates list of year being parsed --------------- #\n",
    "        start_end = [x for x in range(start_year, end_year+1)]\n",
    "\n",
    "        # --- generates list of sitemap urls used to extract articles ---- #\n",
    "        sitemap_urls = []\n",
    "        for year in start_end:\n",
    "            for month in range(1, 13):\n",
    "                if (month < 10):\n",
    "                    sitemap_urls.append(f\"https://www.nytimes.com/sitemaps/new/sitemap-{year}-0{month}.xml.gz\")\n",
    "                else: \n",
    "                    sitemap_urls.append(f\"https://www.nytimes.com/sitemaps/new/sitemap-{year}-{month}.xml.gz\")\n",
    "\n",
    "        # ---------  parses through each sitemap link in sitemap_urls --------- #\n",
    "        for sitemap in sitemap_urls:\n",
    "            page = requests.get(sitemap)\n",
    "            soup = bs(page.content)\n",
    "            links = [link.getText() for link in soup.find_all(\"loc\")]\n",
    "\n",
    "            # -------- parses through all article links in sitemap link -------- #\n",
    "            for link in links:\n",
    "                page = requests.get(link)\n",
    "                temp_soup = bs(page.content)\n",
    "\n",
    "                # ------------------- data being saved to df ------------------- #\n",
    "                # ------- if data non-existent/not found, value = 'None' ------- #\n",
    "\n",
    "                # ----------------- main body of article ----------------------- #\n",
    "                text = \" \".join([p.getText() for p in temp_soup.find_all(\"p\")])\n",
    "\n",
    "                # --- date; saves none to all values if date does not exist/cannot be found --- #\n",
    "                date = re.findall(r'/(\\d{4})/(\\d{1,2})/(\\d{1,2})/', link)\n",
    "                try: \n",
    "                    year = int(date[0][0])\n",
    "                    month = int(date[0][1])\n",
    "                    day = int(date[0][2])\n",
    "                except IndexError:\n",
    "                    year = 'None'\n",
    "                    month = 'None'\n",
    "                    day = 'None'\n",
    "\n",
    "                # ----------- headline; assigns none if no headline --------- #\n",
    "                headline = ''\n",
    "                try:\n",
    "                    headline = temp_soup.find(\"h1\").getText()\n",
    "                except AttributeError:\n",
    "                    headline = 'None'\n",
    "\n",
    "                # ------------- author; assigns none of no topic ------------ #\n",
    "                author = ''\n",
    "                try:\n",
    "                    author = temp_soup.find(\"a\", {'class':'css-mrorfa'}).getText()\n",
    "                except AttributeError:\n",
    "                    author = 'None'\n",
    "\n",
    "                # ------------ topic; assigns none if no topic -------------- #\n",
    "                topic = ''\n",
    "                try:\n",
    "                    topic = temp_soup.find(\"a\", {'class':'css-nuvmzp'}).getText()\n",
    "                except AttributeError:\n",
    "                    topic = 'None'\n",
    "\n",
    "                # ----------------- data appended to df --------------------- #\n",
    "                df = df.append(\n",
    "                    {\n",
    "                        'date': f'{year}-{month}-{day}',\n",
    "                        'headline': headline,\n",
    "                        'author': author,\n",
    "                        'topic': topic,\n",
    "                        'text': text,\n",
    "                        'link': link\n",
    "                        }, ignore_index=True)\n",
    "\n",
    "        # ------------- return created df ------------- #\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- create NYTimesParser object\n",
    "parser = NYTimesParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Parses articles given start and end year ---- #\n",
    "df = parser.parse_articles(start_year = 2018, end_year = 2019)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
